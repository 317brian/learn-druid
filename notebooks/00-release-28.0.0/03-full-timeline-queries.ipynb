{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cb3b009-ebde-4d56-9d59-a028d66d8309",
   "metadata": {},
   "source": [
    "# Query from Deep Storage covering the whole timeline\n",
    "<!--\n",
    "  ~ Licensed to the Apache Software Foundation (ASF) under one\n",
    "  ~ or more contributor license agreements.  See the NOTICE file\n",
    "  ~ distributed with this work for additional information\n",
    "  ~ regarding copyright ownership.  The ASF licenses this file\n",
    "  ~ to you under the Apache License, Version 2.0 (the\n",
    "  ~ \"License\"); you may not use this file except in compliance\n",
    "  ~ with the License.  You may obtain a copy of the License at\n",
    "  ~\n",
    "  ~   http://www.apache.org/licenses/LICENSE-2.0\n",
    "  ~\n",
    "  ~ Unless required by applicable law or agreed to in writing,\n",
    "  ~ software distributed under the License is distributed on an\n",
    "  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "  ~ KIND, either express or implied.  See the License for the\n",
    "  ~ specific language governing permissions and limitations\n",
    "  ~ under the License.\n",
    "  -->\n",
    "Apache Druid is known for fast performance on both historical and real-time data. These queries are resolved through the synchronous API endpoints `/druid/v2` & `/druid/v2/sql` that use Druid's native engine.\n",
    "\n",
    "Asynchronous queries from Deep Storage with API endpoint `/druid/v2/sql/statements` were introduced in the 27.0.0 release along with non-cached Druid retention rules that allow Druid users to query longer timeframes without the need to cache them in the historical layer. This set of features enabled users to optimize their cluster cost by only cacheing recent data in the historical layer and leaving older segment data available for queries in deep storge. \n",
    "\n",
    "At that stage, asynchronous queries were only accessing segments that were avaialble in Deep Storage, so the latest streaming ingested data would not be visible in the query results.\n",
    "\n",
    "With Druid 28.0.0, the asynchronous query capability is expanded to query real-time tasks which allows this type of query to access the complete timeline.\n",
    "\n",
    "This tutorial demonstrates how to work with [Query From Deep Storage](https://druid.apache.org/docs/latest/api-reference/sql-api#query-from-deep-storage). In this tutorial you perform the following tasks:\n",
    "\n",
    "- Generate 3 months of data and use batch ingestion to load it.\n",
    "- Setup stream ingestion for live data from generator.\n",
    "- Setup Retention rules to only hold 1 month of data in Historical cache.\n",
    "- Use synchronous and asynchronous queries to show timeline coverage of each case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdbf6ad-ca7b-40f5-8ca3-1070f4a3ee42",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "This tutorial works with Druid XX.0.0 or later.\n",
    "\n",
    "#### Run with Docker\n",
    "\n",
    "<!-- Profiles are:\n",
    "`druid-jupyter` - just Jupyter and Druid\n",
    "`all-services` - includes Jupyter, Druid, and Kafka\n",
    " -->\n",
    "\n",
    "Launch this tutorial and all prerequisites using the `all-services` profile of the Docker Compose file for Jupyter-based Druid tutorials. For more information, see the Learn Druid repository [readme](https://github.com/implydata/learn-druid).\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5007a243-b81a-4601-8f57-5b14940abbff",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "The following cells set up the notebook and learning environment ready for use.\n",
    "\n",
    "### Set up and connect to the learning environment\n",
    "\n",
    "Run the next cell to set up the Druid Python client's connection to Apache Druid.\n",
    "\n",
    "If successful, the Druid version number will be shown in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ec783b-df3f-4168-9be2-cdc6ad3e33c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import druidapi\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# get druid host from param if available\n",
    "if 'DRUID_HOST' not in os.environ.keys():\n",
    "    druid_host=f\"http://localhost:8888\"\n",
    "else:\n",
    "    druid_host=f\"http://{os.environ['DRUID_HOST']}:8888\"\n",
    "\n",
    "# get kafka host from param if available\n",
    "if 'KAFKA_HOST' not in os.environ.keys():\n",
    "   kafka_host=f\"http://localhost:9092\"\n",
    "else:\n",
    "    kafka_host=f\"{os.environ['KAFKA_HOST']}:9092\"\n",
    "\n",
    "print(f\"Opening a connection to {druid_host}.\")\n",
    "\n",
    "#setup Druid API clients\n",
    "druid = druidapi.jupyter_client(druid_host)\n",
    "display_client = druid.display\n",
    "sql_client = druid.sql\n",
    "status_client = druid.status\n",
    "rest_client = druid.rest\n",
    "\n",
    "# client for Data Generator API\n",
    "datagen = druidapi.rest.DruidRestClient(\"http://datagen:9999\")\n",
    "\n",
    "# define header for REST calls\n",
    "headers = {\n",
    "  'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "status_client.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c40ebc-e4cd-424e-bd68-5ad1ad9cce43",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cd925b-d062-443b-8093-ee0902c5ab01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# wait for the messages to be fully published \n",
    "def wait_for_datagen( job_name): \n",
    "    done = False\n",
    "    while not done:\n",
    "        result = datagen.get_json(f\"/status/{job_name}\",'')\n",
    "        clear_output(wait=True)\n",
    "        print(json.dumps(result, indent=2))\n",
    "        if result[\"status\"] == 'COMPLETE':\n",
    "            done = True\n",
    "        else:\n",
    "            time.sleep(1)\n",
    "\n",
    "\n",
    "# monitor ingestion by counting the rows ingested until the expected number of rows have been loaded\n",
    "def monitor_ingestion( target_table:str, target_rows:int):\n",
    "    row_count=0\n",
    "    while row_count<target_rows:\n",
    "        res = sql_client.sql(f'SELECT count(1) as \"count\" FROM {target_table}')\n",
    "        clear_output(wait=True)\n",
    "        print(json.dumps(res, indent=2))\n",
    "        row_count = res[0]['count']\n",
    "        time.sleep(1)\n",
    "        \n",
    "# suspend the streaming ingestion job and wait for tasks to publish their segments\n",
    "def stop_streaming_job( target_table: str, reset_offsets: bool = False):\n",
    "    print(f'Pause streaming ingestion: [{druid.rest.post(f\"/druid/indexer/v1/supervisor/{target_table}/suspend\",\"\", require_ok=False)}]')\n",
    "    \n",
    "\n",
    "    tasks = druid.tasks.tasks(state='running', table=target_table)\n",
    "    tasks_done = 0\n",
    "    while tasks_done<len(tasks):\n",
    "        tasks_done = 0\n",
    "        clear_output( wait=True)\n",
    "        print(f'Waiting for running tasks to publish their segments ...')\n",
    "        for task in tasks:\n",
    "            status = druid.tasks.task_status(task['id'])\n",
    "            print(f\"Task [{task['id']}] Status:{status['status']['statusCode']} RunnerStatus:{status['status']['runnerStatusCode']}\")\n",
    "            if (status['status']['statusCode']!='RUNNING'): \n",
    "                tasks_done += 1 \n",
    "        time.sleep(1)\n",
    "            \n",
    "    if reset_offsets:\n",
    "        print(f'Reset offsets for re-runnability: [{druid.rest.post(f\"/druid/indexer/v1/supervisor/{target_table}/reset\",\"\", require_ok=False)}]')\n",
    "    print(f'Terminate streaming ingestion: [{druid.rest.post(f\"/druid/indexer/v1/supervisor/{target_table}/terminate\",\"\", require_ok=False)}]')\n",
    "\n",
    "# Remove table data and metadata from Druid\n",
    "def drop_table( target_table: str):\n",
    "    # mark segments as unused \n",
    "    druid.datasources.drop(target_table)\n",
    "    # remove segment metadata and data for unused segments\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    kill_task = {\n",
    "      \"type\": \"kill\",\n",
    "      \"dataSource\": target_table,\n",
    "      \"interval\" : \"2000-09-12/2999-09-13\"\n",
    "    }\n",
    "    print(druid.rest.post(f\"/druid/indexer/v1/task\", json.dumps(kill_task),require_ok=False, headers=headers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472589e4-1026-4b3b-bb79-eedabb2b44c4",
   "metadata": {},
   "source": [
    "## Generate history\n",
    "Run the following cell to create and load 3 months of history up to midnight last night.\n",
    "The data is ingested from this generated data file using SQL Based ingestion.\n",
    "\n",
    "When completed, you'll see a description of the final table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52a94fb-d2e4-403f-ab10-84d3af7bf2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 90 days of click data\n",
    "days_of_history = 90\n",
    "\n",
    "start_time = datetime.now()\n",
    "start_time = start_time - timedelta(days=days_of_history)\n",
    "start_date = start_time.strftime('%Y-%m-%dT%H:%M:%S.001')\n",
    "print(f\"Starting to generate history at {start_date}.\")\n",
    "\n",
    "# Give the datagen job a name for use in subsequent API calls\n",
    "job_name=\"gen_clickstream_history\"\n",
    "\n",
    "# Generate a data file on the datagen server\n",
    "datagen_request = {\n",
    "    \"name\": job_name,\n",
    "    \"target\": { \"type\": \"file\", \"path\":\"clicks-90-days.json\"},\n",
    "    \"config_file\": \"clickstream/clickstream.json\", \n",
    "    \"time_type\": start_date,\n",
    "    \"time\": f\"{days_of_history*24}h\",\n",
    "    \"concurrency\":2\n",
    "}\n",
    "\n",
    "datagen.post(\"/start\", json.dumps(datagen_request), headers=headers, require_ok=False)\n",
    "\n",
    "wait_for_datagen(job_name)\n",
    "\n",
    "# initiate ingestion job\n",
    "sql='''\n",
    "REPLACE INTO \"example-clicks-full-timeline\" OVERWRITE ALL\n",
    "WITH \"ext\" AS (\n",
    "  SELECT *\n",
    "  FROM TABLE(\n",
    "    EXTERN(\n",
    "      '{\"type\":\"http\",\"uris\":[\"http://datagen:9999/file/clicks-90-days.json\"]}',\n",
    "      '{\"type\":\"json\"}'\n",
    "    )\n",
    "  ) EXTEND (\"time\" VARCHAR, \"user_id\" VARCHAR, \"event_type\" VARCHAR, \"client_ip\" VARCHAR, \"client_device\" VARCHAR, \"client_lang\" VARCHAR, \"client_country\" VARCHAR, \"referrer\" VARCHAR, \"keyword\" VARCHAR, \"product\" VARCHAR)\n",
    ")\n",
    "SELECT\n",
    "  TIME_PARSE(\"time\") AS \"__time\",\n",
    "  \"user_id\",\n",
    "  \"event_type\",\n",
    "  \"client_ip\",\n",
    "  \"client_device\",\n",
    "  \"client_lang\",\n",
    "  \"client_country\",\n",
    "  \"referrer\",\n",
    "  \"keyword\",\n",
    "  \"product\"\n",
    "FROM \"ext\"\n",
    "PARTITIONED BY DAY\n",
    "'''\n",
    "\n",
    "display_client.run_task(sql)\n",
    "sql_client.wait_until_ready('example-clicks-full-timeline')\n",
    "display_client.table('example-clicks-full-timeline')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beca41d1-8b6d-4c84-9de9-42be52f09d1e",
   "metadata": {},
   "source": [
    "## Create data stream and streaming ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367f7845-e56a-477a-88de-4fb31b6bd997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give the datagen job a name for use in subsequent API calls\n",
    "job_name=\"gen_clickstream_stream\"\n",
    "\n",
    "# Generate streaming data in real time\n",
    "datagen_request = {\n",
    "    \"name\": job_name,\n",
    "    \"target\": { \"type\": \"kafka\", \"endpoint\": kafka_host, \"topic\": \"clicks\" },\n",
    "    \"config_file\": \"clickstream/clickstream.json\", \n",
    "    \"time_type\": \"REAL\",\n",
    "    \"time\": \"4h\",\n",
    "    \"concurrency\":2\n",
    "}\n",
    "output = datagen.post(\"/start\", json.dumps(datagen_request), headers=headers, require_ok=False)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f314edb-439d-413b-bbc2-7dd965ad24a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start streaming ingestion job\n",
    "kafka_ingestion_spec = {\n",
    "  \"type\": \"kafka\",\n",
    "  \"spec\": {\n",
    "    \"ioConfig\": { \"type\": \"kafka\",  \"consumerProperties\": { \"bootstrap.servers\": \"kafka:9092\" },\n",
    "      \"topic\": \"clicks\",\n",
    "      \"inputFormat\": { \"type\": \"kafka\", \"valueFormat\": { \"type\": \"json\" }   },\n",
    "       \"useEarliestOffset\": True\n",
    "    },\n",
    "    \"tuningConfig\": { \"type\": \"kafka\"  },\n",
    "    \"dataSchema\": {\n",
    "      \"dataSource\": \"example-clicks-full-timeline\",\n",
    "      \"timestampSpec\": { \"column\": \"time\", \"format\": \"iso\" },\n",
    "      \"dimensionsSpec\": {\n",
    "        \"dimensions\": [ ],\n",
    "        \"useSchemaDiscovery\": True\n",
    "      },\n",
    "      \"granularitySpec\": {\n",
    "        \"queryGranularity\": \"none\",\n",
    "        \"rollup\": False,\n",
    "        \"segmentGranularity\": \"day\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "druid.rest.post(\"/druid/indexer/v1/supervisor\", json.dumps(kafka_ingestion_spec), headers=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234e253d-00ee-4e74-8e31-4e724cb0c324",
   "metadata": {},
   "source": [
    "### Nothing new so far\n",
    "\n",
    "It might take it a minute or two to start making the real-time data available. \n",
    "There are 3 months of data in the table and the query next query shows when it has caught up to almost now. Run it a few times until you see the full 90+ days in the data and the most recent event within a minute. Events are being generated irregularly, but there should be at a new one every minute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd200ed0-5d3e-4d0d-ba0c-f4d310e38133",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql='''\n",
    "  SELECT \n",
    "      min(__time) \"min_time\", \n",
    "      max(__time) \"max_time\", \n",
    "      TIMESTAMPDIFF( DAY, min(__time), max(__time)) \"days_of_data\", \n",
    "      TIMESTAMPDIFF( SECOND, CURRENT_TIMESTAMP, max(__time)) \"recent_event_seconds_ago\" \n",
    "  FROM \"example-clicks-full-timeline\"\n",
    "'''\n",
    "display_client.sql(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc7e74f-fdd5-4b79-9c83-c74b6142aa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(datagen.get(f\"/status/gen_clickstream_stream\", '').json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8662a937-71cc-4b65-95cf-ea2c4943f947",
   "metadata": {},
   "source": [
    "## Retention rules\n",
    "\n",
    "Retention rules can be defined to only cache a portion of the timeline on historical servers.\n",
    "A Load rule with an empty `\"tieredReplicants\": {}` , tells Druid that segment files which fall in this timeframe will not have any copies on historical tiers, but such segments will be kept in deep storage and are queryable with asynchronous queries that use MSQ.\n",
    "\n",
    "The following cell, configures the retention rules to:\n",
    "- keep 1 month of data in the historical layer\n",
    "- keep another 3 months of data in deep storage\n",
    "- remove anything older than 3 months\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa02315-6353-4cea-a6af-4d1fa90786f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "retention_rule = [\n",
    "    {\"type\":\"loadByPeriod\", \"period\":\"P1M\", \"tieredReplicants\": { \"_default_tier\": 1} },\n",
    "    {\"type\":\"loadByPeriod\", \"period\":\"P3M\", \"tieredReplicants\": { }, \"useDefaultTierForNull\": False }, \n",
    "    {\"type\":\"dropForever\" }\n",
    "]\n",
    "\n",
    "druid.rest.post(\"/druid/coordinator/v1/rules/example-clicks-full-timeline\", json.dumps(retention_rule), headers=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9376fc69-969b-4fe0-a408-79ad218aa4f1",
   "metadata": {},
   "source": [
    "## Queries with native engine \n",
    "It might take a minute or two for the cluster to re-organize the data.\n",
    "\n",
    "The coordinator will ask the historical servers to offload any copied of segment data that are older than 1 month ago from the first rule's period `P1M`. \n",
    "\n",
    "Try the following cell multiple times until you see this result.\n",
    "- `min_time` will now be 30 days ago \n",
    "- `max_time` will continue to keep up with real-time.\n",
    "- `days_of_data` will now report 31 days, the past 30 plus the one we continue to load\n",
    "- `most_recev_to_now_s` should be up to date, events occur about 1 every minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fcf5d7-5f0c-432c-8319-29c7060c0912",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql='''\n",
    "  SELECT \n",
    "      min(__time) \"min_time\", \n",
    "      max(__time) \"max_time\", \n",
    "      TIMESTAMPDIFF( DAY, min(__time), max(__time)) \"days_of_data\", \n",
    "      TIMESTAMPDIFF( SECOND, CURRENT_TIMESTAMP, max(__time)) \"most_recent_to_now_s\" \n",
    "  FROM \"example-clicks-full-timeline\"\n",
    "'''\n",
    "display_client.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05078320-e760-4556-9746-4937b0335ace",
   "metadata": {},
   "source": [
    "## Queries with MSQ \n",
    "Queries using the asynchronous API `/druid/v2/sql/statements` run on the MSQ engine which reads data directly from deep storage and therefore has all 3 months available.\n",
    "\n",
    "That was a long setup to describe the new feature in Druid 28.0.0!\n",
    "\n",
    "Run the following SQL to see that async queries cover the whole timeframe, including real-time. \n",
    "\n",
    "This is the new feature:\n",
    "- set the query context parameter `includeSegmentSource=REALTIME`\n",
    "- asynchronous queries will all query the real-time tasks\n",
    "\n",
    "Try without setting it first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2824a1b7-b374-4464-b5b6-2dc17a75fcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql='''\n",
    "  SELECT \n",
    "      TIME_FORMAT( min(__time), 'YYYY-MM-dd hh:mm:ss') \"min_time\", \n",
    "      TIME_FORMAT(max(__time), 'YYYY-MM-dd hh:mm:ss') \"max_time\", \n",
    "      TIMESTAMPDIFF( DAY, min(__time), max(__time)) \"days_of_data\", \n",
    "      TIMESTAMPDIFF( SECOND, CURRENT_TIMESTAMP, max(__time)) \"most_recent_to_now_s\" \n",
    "  FROM \"example-clicks-full-timeline\"\n",
    "'''\n",
    "result = sql_client.async_sql(sql)\n",
    "display(result.rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8db109-3cf5-40e6-91db-3ac844a57c22",
   "metadata": {},
   "source": [
    "Notice that the query did access all the ingested segments starting from the date 90 days ago.\n",
    "Now try with `includeSegmentSource = REALTIME`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8761d86c-98d8-4d99-b7ad-fa6c1a2e8aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql='''\n",
    "  SELECT \n",
    "      TIME_FORMAT( min(__time), 'YYYY-MM-dd hh:mm:ss') \"min_time\", \n",
    "      TIME_FORMAT(max(__time), 'YYYY-MM-dd hh:mm:ss') \"max_time\", \n",
    "      TIMESTAMPDIFF( DAY, min(__time), max(__time)) \"days_of_data\", \n",
    "      TIMESTAMPDIFF( SECOND, CURRENT_TIMESTAMP, max(__time)) \"most_recent_to_now_s\" \n",
    "  FROM \"example-clicks-full-timeline\"\n",
    "'''\n",
    "req = sql_client.sql_request(sql)\n",
    "req.add_context(\"includeSegmentSource\", \"realtime\")\n",
    "result = sql_client.async_sql(req)\n",
    "display(result.rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44738d6d-cec2-40ad-aaba-998c758c63f4",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "Run the following cell to remove everything used in this notebook from the database and data generation engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8082b545-ba7f-4ede-bb6e-2a6dd62ba0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stop_streaming_job(\"example-clicks-full-timeline\")\n",
    "\n",
    "display(datagen.post(f\"/stop/gen_clickstream_stream\", '', require_ok=False).json())\n",
    "display(datagen.post(f\"/stop/gen_clickstream_history\", '', require_ok=False).json())\n",
    "\n",
    "drop_table(\"example-clicks-full-timeline\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b8d5fe-ba85-4b5b-9669-0dd47dfbccd1",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You learned about setting up retention rules for different periods to:\n",
    "* cache recent segments in historical tier\n",
    "* keep older segments available for async queries from deep storage\n",
    "* use async queries that also retrieve real-time data"
   ]
  }
 ],
 "metadata": {
  "execution": {
   "allow_errors": true,
   "timeout": 300
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
